{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MultiRD",
   "id": "9d85d4123d01cc83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# set GDRIVE_SAVE_DIR=\"\" to disable export of model checkpoint file to Google Drive\n",
    "# if GDRIVE_SAVE_DIR is specified, the first path component is required to be 'MyDrive'\n",
    "GDRIVE_SAVE_DIR = \"MyDrive/CS-GY 6953 DL/DL final project/checkpoints\"\n",
    "# GDRIVE_SAVE_DIR = \"\"\n",
    "\n",
    "class Uploader:\n",
    "\n",
    "    def __init__(self, local_gdrive_save_path: Optional[str] = None):\n",
    "        self.local_gdrive_save_path = local_gdrive_save_path\n",
    "        self.conservation_deck = {}\n",
    "    \n",
    "    def is_enabled(self) -> bool:\n",
    "        return True if self.local_gdrive_save_path else False\n",
    "    \n",
    "    def replace(self, saved_file: Path, conserve: Optional[str] = None):\n",
    "        if not conserve:\n",
    "            return\n",
    "        previous = self.conservation_deck.get(conserve, None)\n",
    "        if previous is not None:\n",
    "            try:\n",
    "                os.remove(previous)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        self.conservation_deck[conserve] = saved_file\n",
    "                \n",
    "    def upload_file(self, src_file: Path, dst_path: str, suppress_error: bool = False, conserve: Optional[str] = None) -> Optional[str]:\n",
    "        if not self.local_gdrive_save_path:\n",
    "            return\n",
    "        dst_file = Path(self.local_gdrive_save_path) / dst_path\n",
    "        try:\n",
    "            dst_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "            shutil.copyfile(src_file, dst_file)\n",
    "            self.replace(dst_file, conserve=conserve)\n",
    "            return str(dst_file)\n",
    "        except Exception as e:\n",
    "            if suppress_error:\n",
    "                print(f\"suppressing save error {type(e)} {e} on file {src_file} -> {dst_file}\", file=sys.stderr)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def upload_checkpoint(self, checkpoint_file: Path, infix: str) -> Optional[str]:\n",
    "        if not self.local_gdrive_save_path:\n",
    "            return\n",
    "        filename = f\"{checkpoint_file.stem}-{infix}{checkpoint_file.suffix}\"\n",
    "        dst_file = self.upload_file(checkpoint_file, filename)\n",
    "        return dst_file\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_mount() -> 'Uploader':\n",
    "        save_path_root = \"/content/gdrive\"\n",
    "        local_save_root = str(os.path.join(save_path_root, GDRIVE_SAVE_DIR))\n",
    "        if GDRIVE_SAVE_DIR:\n",
    "            try:\n",
    "                # noinspection PyUnresolvedReferences\n",
    "                from google.colab import drive\n",
    "                drive.mount(save_path_root)\n",
    "                return Uploader(local_save_root)\n",
    "            except Exception as e:\n",
    "                if isinstance(e, ImportError):\n",
    "                    print(\"(not saving because not in colab environment)\")\n",
    "                else:\n",
    "                    print(\"not saving to gdrive due to\", type(e).__name__, e)\n",
    "        return Uploader()  # not enabled\n",
    "\n",
    "UPLOADER = Uploader.prepare_mount()"
   ],
   "id": "dfa1d532cf5ab5ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# unpack data\n",
    "!(test -d data || (gdown \"1zjrLaaKR9Pf-DUmjkoptRyG4SBasgked\" && unzip -q \"english-rd-data.zip\"))\n",
    "!ls data\n",
    "DATA_PATH = './data'"
   ],
   "id": "5d7b696edfbd4517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "data.py",
   "id": "13848d4e2669b03e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.cuda\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from typing import TextIO\n",
    "from typing import Any\n",
    "from typing import NamedTuple\n",
    "from typing import Sequence\n",
    "from json import load as json_load\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, instances):\n",
    "        self.instances = instances\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.instances[index]\n",
    " \n",
    "def data2index(data_x, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"word\": \"restlessly\",\n",
    "        \"lexnames\": [\n",
    "            \"adv.all\"\n",
    "        ],\n",
    "        \"root_affix\": [\n",
    "            \"ly\"\n",
    "        ],\n",
    "        \"sememes\": [\n",
    "            \"rash\"\n",
    "        ],\n",
    "        \"definitions\": \"in a restless manner unquietly\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    data_x_idx = list()\n",
    "    for instance in data_x:\n",
    "        sememe_idx = [sememe2index[se] for se in instance['sememes']]\n",
    "        lexname_idx = [lexname2index[ln] for ln in instance['lexnames']]\n",
    "        rootaffix_idx = [rootaffix2index[ra] for ra in instance['root_affix'] if rootaffix_freq[ra]>=frequency]\n",
    "        def_word_idx = list()\n",
    "        def_words = instance['definitions'].strip().split()\n",
    "        if len(def_words) > 0:\n",
    "            for def_word in def_words:\n",
    "                if def_word in word2index and def_word!=instance['word']:\n",
    "                    def_word_idx.append(word2index[def_word])\n",
    "                else:\n",
    "                    def_word_idx.append(word2index['<OOV>'])\n",
    "            data_x_idx.append({'word': word2index[instance['word']], 'lexnames': lexname_idx, 'root_affix':rootaffix_idx, 'sememes': sememe_idx, 'definition_words': def_word_idx})\n",
    "        else:\n",
    "            pass #print(instance['word'], instance['definitions']) # some is null\n",
    "    return data_x_idx\n",
    "\n",
    "\n",
    "def readlines(f: TextIO):\n",
    "    return f.readlines()\n",
    "\n",
    "\n",
    "def load_data_file(filename: str, transform: Callable[[TextIO], Any]):\n",
    "    with open(os.path.join(DATA_PATH, filename)) as ifile:\n",
    "        if transform is not None:\n",
    "            return transform(ifile)\n",
    "        return ifile.read()\n",
    "\n",
    "def load_data_json(filename: str):\n",
    "    return load_data_file(filename, transform=json_load)\n",
    "\n",
    "def load_data_lines(filename: str):\n",
    "    return load_data_file(filename, transform=readlines)\n",
    "\n",
    "\n",
    "class WordMappings(NamedTuple):\n",
    "    \n",
    "    word2index: dict[str, int]\n",
    "    index2word: list[str]\n",
    "    word2vec: np.ndarray\n",
    "    \n",
    "    def describe(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"word2index\": len(self.word2index),\n",
    "            \"index2word\": len(self.index2word),\n",
    "            \"word2vec\": f\"{self.word2vec.shape} {self.word2vec.dtype}\",\n",
    "        }\n",
    "    \n",
    "    def expand(self, phrase_indexes: Sequence[int]) -> str:\n",
    "        words = [self.index2word[word_i] for word_i in phrase_indexes]\n",
    "        stop = len(words)\n",
    "        for i in range(len(words)):\n",
    "            if words[i] == '<PAD>':\n",
    "                stop = i\n",
    "                break\n",
    "            if words[i] == '<OOV>':\n",
    "                words[i] = '#'\n",
    "        return \" \".join(words[:stop])\n",
    "            \n",
    "\n",
    "\n",
    "class CoreMappings(NamedTuple):\n",
    "    \n",
    "    index2sememe: list[str]\n",
    "    index2lexname: list[str]\n",
    "    index2rootaffix: list[str]\n",
    "    \n",
    "    def describe(self) -> dict[str, int]:\n",
    "        return dict((k, len(v)) for k, v in self._asdict().items())\n",
    "\n",
    "\n",
    "class Label(NamedTuple):\n",
    "    \n",
    "    size: int\n",
    "    lexname_size: int\n",
    "    rootaffix_size: int\n",
    "    sememe_size: int\n",
    "\n",
    "\n",
    "class Indexes(NamedTuple):\n",
    "    \n",
    "    data_train_idx: list[int]\n",
    "    data_dev_idx: list[int]\n",
    "    data_test_500_seen_idx: list[int]\n",
    "    data_test_500_unseen_idx: list[int]\n",
    "    data_defi_c_idx: list[int]\n",
    "    data_desc_c_idx: list[int]\n",
    "\n",
    "\n",
    "class LoadedData(NamedTuple):\n",
    "    \n",
    "    word_mappings: WordMappings\n",
    "    core_mappings: CoreMappings\n",
    "    label: Label\n",
    "    indexes: Indexes\n",
    "\n",
    "\n",
    "def load_data(frequency: int) -> LoadedData:\n",
    "    print('Loading dataset...')\n",
    "    data_train = load_data_json(\"data_train.json\")\n",
    "    data_dev = load_data_json(\"data_dev.json\")\n",
    "    data_test_500_rand1_seen = load_data_json(\"data_test_500_rand1_seen.json\")\n",
    "    data_test_500_rand1_unseen = load_data_json(\"data_test_500_rand1_unseen.json\") #data_test_500_others\n",
    "    data_defi_c = load_data_json(\"data_defi_c.json\")\n",
    "    data_desc_c = load_data_json(\"data_desc_c.json\")\n",
    "    lines = load_data_lines(\"target_words.txt\")\n",
    "    target_words = [line.strip() for line in lines]\n",
    "    label_size = len(target_words)+2\n",
    "    print('target_words (include <PAD><OOV>): ', label_size)\n",
    "    lines = load_data_lines(\"lexname_all.txt\")\n",
    "    lexname_all = [line.strip() for line in lines]\n",
    "    label_lexname_size = len(lexname_all)\n",
    "    print('label_lexname_size: ', label_lexname_size)\n",
    "    lines = load_data_lines(\"root_affix_freq.txt\")\n",
    "    rootaffix_freq = {}\n",
    "    for line in lines:\n",
    "        rootaffix_freq[line.strip().split()[0]] = int(line.strip().split()[1])\n",
    "    lines = load_data_lines(\"rootaffix_all.txt\")\n",
    "    rootaffix_all = [line.strip() for line in lines]\n",
    "    lines = load_data_lines(\"sememes_all.txt\")\n",
    "    sememes_all = [line.strip() for line in lines]\n",
    "    label_sememe_size = len(sememes_all)+1\n",
    "    print('label_sememe_size: ', label_sememe_size)\n",
    "    vec_inuse = load_data_json(\"vec_inuse.json\")\n",
    "    vocab = list(vec_inuse)\n",
    "    vocab_size = len(vocab)+2\n",
    "    print('vocab (embeddings in use)(include <PAD><OOV>): ', vocab_size)\n",
    "    word2index: dict[str, int] = dict()\n",
    "    index2word: list[str] = list()\n",
    "    word2index['<PAD>'] = 0\n",
    "    word2index['<OOV>'] = 1\n",
    "    index2word.extend(['<PAD>', '<OOV>'])\n",
    "    index2word.extend(vocab)\n",
    "    word2vec = np.zeros((vocab_size, len(list(vec_inuse.values())[0])), dtype=np.float32)\n",
    "    for wd in target_words: \n",
    "        index = len(word2index)\n",
    "        word2index[wd] = index\n",
    "        word2vec[index, :] = vec_inuse[wd]\n",
    "    for wd in vocab:\n",
    "        if wd in target_words:\n",
    "            continue\n",
    "        index = len(word2index)\n",
    "        word2index[wd] = index\n",
    "        word2vec[index, :] = vec_inuse[wd]\n",
    "    sememe2index = dict()\n",
    "    index2sememe = list()\n",
    "    for sememe in sememes_all:\n",
    "        sememe2index[sememe] = len(sememe2index)\n",
    "        index2sememe.append(sememe)\n",
    "    lexname2index = dict()\n",
    "    index2lexname = list()\n",
    "    for ln in lexname_all:\n",
    "        lexname2index[ln] = len(lexname2index)\n",
    "        index2lexname.append(ln)\n",
    "    rootaffix2index = dict()\n",
    "    index2rootaffix = list()\n",
    "    for ra in rootaffix_all:\n",
    "        if rootaffix_freq[ra] >= frequency:\n",
    "            rootaffix2index[ra] = len(rootaffix2index)\n",
    "            index2rootaffix.append(ra)\n",
    "    label_rootaffix_size = len(index2rootaffix)\n",
    "    print('label_rootaffix_size: ', label_rootaffix_size)\n",
    "    data_train_idx = data2index(data_train, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    print('data_train size: %d'%len(data_train_idx))\n",
    "    data_dev_idx = data2index(data_dev, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    print('data_dev size: %d'%len(data_dev_idx))\n",
    "    data_test_500_seen_idx = data2index(data_test_500_rand1_seen, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency) \n",
    "    print('data_test_seen size: %d'%len(data_test_500_seen_idx))\n",
    "    data_test_500_unseen_idx = data2index(data_test_500_rand1_unseen, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency) \n",
    "    print('data_test_unseen size: %d'%len(data_test_500_unseen_idx))\n",
    "    data_defi_c_idx = data2index(data_defi_c, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    data_desc_c_idx = data2index(data_desc_c, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)    \n",
    "    print('data_desc size: %d'%len(data_desc_c_idx))\n",
    "    return LoadedData(\n",
    "        word_mappings=WordMappings(word2index, index2word, word2vec),\n",
    "        core_mappings=CoreMappings(index2sememe, index2lexname, index2rootaffix),\n",
    "        label=Label(label_size, label_lexname_size, label_rootaffix_size, label_sememe_size),\n",
    "        indexes=Indexes(data_train_idx, data_dev_idx, data_test_500_seen_idx, data_test_500_unseen_idx, data_defi_c_idx, data_desc_c_idx),\n",
    "    )\n",
    "    # return word2index, index2word, word2vec, (index2sememe, index2lexname, index2rootaffix), (label_size, label_lexname_size, label_rootaffix_size, label_sememe_size), (data_train_idx, data_dev_idx, data_test_500_seen_idx, data_test_500_unseen_idx, data_defi_c_idx, data_desc_c_idx)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def build_sentence_numpy(sentences):\n",
    "    max_length = max([len(sentence) for sentence in sentences])\n",
    "    sentence_numpy = np.zeros((len(sentences), max_length), dtype=np.int64)\n",
    "    for i in range(len(sentences)):\n",
    "        sentence_numpy[i, 0:len(sentences[i])] = np.array(sentences[i])\n",
    "    return sentence_numpy\n",
    "    \n",
    "\n",
    "def label_multihot(labels, num):\n",
    "    sm = np.zeros((len(labels), num), dtype=np.float32)\n",
    "    for i in range(len(labels)):\n",
    "        for s in labels[i]:\n",
    "            if s >= num:\n",
    "                break\n",
    "            sm[i, s] = 1\n",
    "    return sm\n",
    "    \n",
    "def my_collate_fn(batch):\n",
    "    words = [instance['word'] for instance in batch]\n",
    "    definition_words = [instance['definition_words'] for instance in batch]\n",
    "    words_t = torch.tensor(np.array(words), dtype=torch.int64, device=device)\n",
    "    definition_words_t = torch.tensor(build_sentence_numpy(definition_words), dtype=torch.int64, device=device)\n",
    "    return words_t, definition_words_t\n",
    "    \n",
    "def word2feature(dataset, word_num, feature_num, feature_name):\n",
    "    max_feature_num = max([len(instance[feature_name]) for instance in dataset])\n",
    "    ret = np.zeros((word_num, max_feature_num), dtype=np.int64)\n",
    "    ret.fill(feature_num)\n",
    "    for instance in dataset:\n",
    "        if ret[instance['word'], 0] != feature_num: \n",
    "            continue # this target_words has been given a feature mapping, because same word with different definition in dataset\n",
    "        feature = instance[feature_name]\n",
    "        ret[instance['word'], :len(feature)] = np.array(feature)\n",
    "    return torch.tensor(ret, dtype=torch.int64, device=device)\n",
    "    \n",
    "def mask_noFeature(label_size, wd2fea, feature_num):\n",
    "    mask_nofea = torch.zeros(label_size, dtype=torch.float32, device=device)\n",
    "    for i in range(label_size):\n",
    "        feas = set(wd2fea[i].detach().cpu().numpy().tolist()) - {feature_num}\n",
    "        if len(feas)==0:\n",
    "            mask_nofea[i] = 1\n",
    "    return mask_nofea\n"
   ],
   "id": "5751f965237d32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "model.py",
   "id": "1daeb8e189ab5d23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        # x: T(bat, len, emb) float32\n",
    "        # x_len: T(bat) int64\n",
    "        _, x_len_sort_idx = torch.sort(-x_len)\n",
    "        _, x_len_unsort_idx = torch.sort(x_len_sort_idx)\n",
    "        x = x[x_len_sort_idx]\n",
    "        x_len = x_len[x_len_sort_idx]\n",
    "        x_len = x_len.to(\"cpu\")\n",
    "        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True)\n",
    "        # ht: T(num_layers*2, bat, hid) float32\n",
    "        # ct: T(num_layers*2, bat, hid) float32\n",
    "        h_packed, (ht, ct) = self.lstm(x_packed, None)\n",
    "        ht = ht[:, x_len_unsort_idx, :]\n",
    "        ct = ct[:, x_len_unsort_idx, :]\n",
    "        # h: T(bat, len, hid*2) float32\n",
    "        h, _ = torch.nn.utils.rnn.pad_packed_sequence(h_packed, batch_first=True)\n",
    "        h = h[x_len_unsort_idx]\n",
    "        return h, (ht, ct)\n",
    "        \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, layers, class_num, sememe_num, lexname_num, rootaffix_num):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.class_num = class_num\n",
    "        self.sememe_num = sememe_num\n",
    "        self.lexname_num = lexname_num\n",
    "        self.rootaffix_num = rootaffix_num\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0, max_norm=5, sparse=True)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = torch.nn.Dropout()\n",
    "        self.encoder = BiLSTM(self.embed_dim, self.hidden_dim, self.layers)\n",
    "        self.fc = torch.nn.Linear(self.hidden_dim*2, self.embed_dim)\n",
    "        self.fc_s = torch.nn.Linear(self.hidden_dim*2, self.sememe_num)\n",
    "        self.fc_l = torch.nn.Linear(self.hidden_dim*2, self.lexname_num)\n",
    "        self.fc_r = torch.nn.Linear(self.hidden_dim*2, self.rootaffix_num)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, operation, x=None, w=None, ws=None, wl=None, wr=None, msk_s=None, msk_l=None, msk_r=None, mode=None):\n",
    "        # x: T(bat, max_word_num)\n",
    "        # w: T(bat)\n",
    "        # x_embedding: T(bat, max_word_num, embed_dim)\n",
    "        x_embedding = self.embedding(x)\n",
    "        x_embedding = self.embedding_dropout(x_embedding)\n",
    "        # mask: T(bat, max_word_num)\n",
    "        mask = torch.gt(x, 0).to(torch.int64)\n",
    "        # x_len: T(bat)\n",
    "        x_len = torch.sum(mask, dim=1)\n",
    "        # h: T(bat, max_word_num, hid*2)\n",
    "        # ht: T(num_layers*2, bat, hid) float32\n",
    "        h, (ht, _) = self.encoder(x_embedding, x_len)\n",
    "        # ht: T(bat, hid*2)\n",
    "        ht = torch.transpose(ht[ht.shape[0] - 2:, :, :], 0, 1).contiguous().view(x_len.shape[0], self.hidden_dim*2)\n",
    "        # alpha: T(bat, max_word_num, 1)\n",
    "        alpha = (h.bmm(ht.unsqueeze(2)))\n",
    "        # mask_3: T(bat, max_word_num, 1)\n",
    "        mask_3 = mask.to(torch.float32).unsqueeze(2)\n",
    "\n",
    "        ## word prediction\n",
    "        # vd: T(bat, embed_dim)\n",
    "        h_1 = torch.sum(h*alpha, 1)\n",
    "        vd = self.fc(h_1) #+ torch.sum(self.embedding(x), 1)#+ torch.sum(x_embedding, 1) #ok\n",
    "        #vd = self.fc(torch.sum(torch.cat([h, self.embedding(x)], 2)*alpha, 1)) #best\n",
    "        score0 = vd.mm(self.embedding.weight[[range(self.class_num)]].t())\n",
    "        score = score0\n",
    "        if 's' in mode:\n",
    "            ## sememe prediction\n",
    "            # pos_score: T(bat, max_word_num, sememe_num)\n",
    "            pos_score = self.fc_s(h)\n",
    "            pos_score = pos_score*mask_3 + (-1e7)*(1-mask_3)\n",
    "            # sem_score: T(bat, sememe_num)\n",
    "            sem_score, _ = torch.max(pos_score, dim=1)\n",
    "            #sem_score = torch.sum(pos_score * alpha, 1)\n",
    "            # score: T(bat, class_num) = [bat, sememe_num] .mm [class_num, sememe_num].t()\n",
    "            score_s = self.relu(sem_score.mm(ws.t()))\n",
    "            #----------add mean sememe score to those who have no sememes\n",
    "            # mean_sem_sc: T(bat)\n",
    "            mean_sem_sc = torch.mean(score_s, 1)\n",
    "            # msk: T(class_num)\n",
    "            score_s = score_s + mean_sem_sc.unsqueeze(1).mm(msk_s.unsqueeze(0))\n",
    "            #----------\n",
    "            score = score + score_s\n",
    "        if 'r' in mode:\n",
    "            ## root-affix prediction\n",
    "            pos_score_ = self.fc_r(h)\n",
    "            pos_score_ = pos_score_*mask_3 + (-1e7)*(1-mask_3)\n",
    "            ra_score, _ = torch.max(pos_score_, dim=1)\n",
    "            score_r = self.relu(ra_score.mm(wr.t()))\n",
    "            mean_ra_sc = torch.mean(score_r, 1)\n",
    "            score_r = score_r + mean_ra_sc.unsqueeze(1).mm(msk_r.unsqueeze(0))\n",
    "            score = score + score_r\n",
    "        if 'l' in mode:\n",
    "            ## lexname prediction\n",
    "            lex_score = self.fc_l(h_1)\n",
    "            score_l = self.relu(lex_score.mm(wl.t()))\n",
    "            mean_lex_sc = torch.mean(score_l, 1)\n",
    "            score_l = score_l + mean_lex_sc.unsqueeze(1).mm(msk_l.unsqueeze(0))\n",
    "            score = score + score_l\n",
    "        \n",
    "        # fine-tune depended on the target word shouldn't exist in the definition.\n",
    "        #score_res = score.clone().detach()\n",
    "        mask1 = torch.lt(x, self.class_num).to(torch.int64)\n",
    "        mask2 = torch.ones((score.shape[0], score.shape[1]), dtype=torch.float32, device=device)\n",
    "        for i in range(x.shape[0]):\n",
    "            mask2[i][x[i]*mask1[i]] = 0.\n",
    "        score = score * mask2 + (-1e6)*(1-mask2)\n",
    "        \n",
    "        _, indices = torch.sort(score, descending=True)\n",
    "        if operation == 'train':\n",
    "            loss = self.loss(score, w)\n",
    "            return loss, score, indices\n",
    "        elif operation == 'test':\n",
    "            return indices\n"
   ],
   "id": "5bf99c9d404fd1ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "evaluate.py",
   "id": "29457543a1b3afac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate(ground_truth, prediction):\n",
    "    accu_1 = 0.\n",
    "    accu_10 = 0.\n",
    "    accu_100 = 0.\n",
    "    length = len(ground_truth)\n",
    "    for i in range(length):\n",
    "        if ground_truth[i] in prediction[i][:100]:\n",
    "            accu_100 += 1\n",
    "            if ground_truth[i] in prediction[i][:10]:\n",
    "                accu_10 += 1\n",
    "                if ground_truth[i] == prediction[i][0]:\n",
    "                    accu_1 += 1\n",
    "    return accu_1/length*100, accu_10/length*100, accu_100/length*100\n",
    "\n",
    "def evaluate_test(ground_truth, prediction):\n",
    "    accu_1 = 0.\n",
    "    accu_10 = 0.\n",
    "    accu_100 = 0.\n",
    "    length = len(ground_truth)\n",
    "    pred_rank = []\n",
    "    for i in range(length):\n",
    "        try:\n",
    "            pred_rank.append(prediction[i][:].index(ground_truth[i]))\n",
    "        except:\n",
    "            pred_rank.append(1000)\n",
    "        if ground_truth[i] in prediction[i][:100]:\n",
    "            accu_100 += 1\n",
    "            if ground_truth[i] in prediction[i][:10]:\n",
    "                accu_10 += 1\n",
    "                if ground_truth[i] == prediction[i][0]:\n",
    "                    accu_1 += 1\n",
    "    return accu_1/length*100, accu_10/length*100, accu_100/length*100, np.median(pred_rank), np.sqrt(np.var(pred_rank))\n",
    "\n",
    "# '''\n",
    "# def evaluate_MAP(ground_truth, prediction):\n",
    "#     index = 1\n",
    "#     correct = 0\n",
    "#     point = 0\n",
    "#     for predicted_POS in prediction:\n",
    "#         if predicted_POS in ground_truth:\n",
    "#             correct += 1\n",
    "#             point += (correct / index)\n",
    "#         index += 1\n",
    "#     point /= len(ground_truth)\n",
    "#     return point*100.\n",
    "# \n",
    "# import numpy as np    \n",
    "# def evaluate1(ground_truth, prediction):\n",
    "#     length = len(ground_truth)\n",
    "#     ref = np.array(ground_truth)[:, np.newaxis]\n",
    "#     _, c = np.where(np.array(prediction)==ref)\n",
    "#     accu_1 = np.sum(c==0)\n",
    "#     accu_10 = np.sum(c<10)\n",
    "#     accu_100 = np.sum(c<100)\n",
    "#     return accu_1/length*100, accu_10/length*100, accu_100/length*100\n",
    "# '''"
   ],
   "id": "5fd013aa6561077a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Something else?",
   "id": "2367a9ca33c3f4dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TBD",
   "id": "389f6b38618c70bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "main.py",
   "id": "2aaa8600a06f9225"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import NamedTuple\n",
    "from torch import Tensor\n",
    "\n",
    "class Counts(NamedTuple):\n",
    "    \n",
    "    train: int\n",
    "    valid: int\n",
    "    test: int\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    \n",
    "    train_dataset: MyDataset\n",
    "    valid_dataset: MyDataset\n",
    "    test_dataset: MyDataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_loaded(loaded_data: LoadedData):\n",
    "        test_dataset = MyDataset(loaded_data.indexes.data_test_500_seen_idx + loaded_data.indexes.data_test_500_unseen_idx + loaded_data.indexes.data_desc_c_idx)\n",
    "        valid_dataset = MyDataset(loaded_data.indexes.data_dev_idx)\n",
    "        train_dataset = MyDataset(loaded_data.indexes.data_train_idx + loaded_data.indexes.data_defi_c_idx)\n",
    "        return Split(train_dataset, valid_dataset, test_dataset)\n",
    "\n",
    "\n",
    "class Loaders(NamedTuple):\n",
    "    \n",
    "    train_dataloader: torch.utils.data.DataLoader\n",
    "    valid_dataloader: torch.utils.data.DataLoader\n",
    "    test_dataloader: torch.utils.data.DataLoader\n",
    "    counts: Counts\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_split(datasets: Split, batch_size: int) -> 'Loaders':\n",
    "        train_dataloader = torch.utils.data.DataLoader(datasets.train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "        valid_dataloader = torch.utils.data.DataLoader(datasets.valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "        test_dataloader = torch.utils.data.DataLoader(datasets.test_dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "        return Loaders(train_dataloader, valid_dataloader, test_dataloader, Counts(len(datasets.train_dataset), len(datasets.valid_dataset), len(datasets.test_dataset)))\n",
    "\n",
    "class Preprocessed(NamedTuple):\n",
    "    \n",
    "    wd_sems: Tensor\n",
    "    wd_lex: Tensor\n",
    "    wd_ra: Tensor\n",
    "    mask_s: Tensor\n",
    "    mask_l: Tensor\n",
    "    mask_r: Tensor\n",
    "    \n",
    "    def to(self, pt_device: str):\n",
    "        d = self._asdict()\n",
    "        for k, value in d.items():\n",
    "            value: Tensor\n",
    "            value = value.to(pt_device)\n",
    "            d[k] = value\n",
    "        return Preprocessed(**d)\n",
    "    \n",
    "    def describe(self) -> dict[str, torch.Size]:\n",
    "        return dict((k, v.shape) for k, v in self._asdict().items())\n",
    "\n",
    "\n",
    "def prepare_data(loaded_data: LoadedData) -> Preprocessed:\n",
    "    # word2index, index2word, word2vec, index2each, label_size_each, data_idx_each = load_data(frequency)\n",
    "    \n",
    "    (label_size, label_lexname_size, label_rootaffix_size, label_sememe_size) = loaded_data.label\n",
    "    (data_train_idx, data_dev_idx, data_test_500_seen_idx, data_test_500_unseen_idx, data_defi_c_idx, data_desc_c_idx) = loaded_data.indexes\n",
    "    (index2sememe, index2lexname, index2rootaffix) = loaded_data.core_mappings\n",
    "    \n",
    "    sp = Split.from_loaded(loaded_data)\n",
    "    \n",
    "    print('Train dataset: ', len(sp.train_dataset))\n",
    "    print('Valid dataset: ', len(sp.valid_dataset))\n",
    "    print('Test dataset: ', len(sp.test_dataset))\n",
    "    data_all_idx = data_train_idx + data_dev_idx + data_test_500_seen_idx + data_test_500_unseen_idx + data_defi_c_idx\n",
    "    \n",
    "    sememe_num = len(index2sememe)\n",
    "    wd2sem = word2feature(data_all_idx, label_size, sememe_num, 'sememes') # label_size, not len(word2index). we only use target_words' feature\n",
    "    wd_sems = label_multihot(wd2sem, sememe_num)\n",
    "    wd_sems = torch.from_numpy(np.array(wd_sems)).to(device) #torch.from_numpy(np.array(wd_sems[:label_size])).to(device)\n",
    "    lexname_num = len(index2lexname)\n",
    "    wd2lex = word2feature(data_all_idx, label_size, lexname_num, 'lexnames') \n",
    "    wd_lex = label_multihot(wd2lex, lexname_num)\n",
    "    wd_lex = torch.from_numpy(np.array(wd_lex)).to(device)\n",
    "    rootaffix_num = len(index2rootaffix)\n",
    "    wd2ra = word2feature(data_all_idx, label_size, rootaffix_num, 'root_affix') \n",
    "    wd_ra = label_multihot(wd2ra, rootaffix_num)\n",
    "    wd_ra = torch.from_numpy(np.array(wd_ra)).to(device)\n",
    "    mask_s = mask_noFeature(label_size, wd2sem, sememe_num)\n",
    "    mask_l = mask_noFeature(label_size, wd2lex, lexname_num)\n",
    "    mask_r = mask_noFeature(label_size, wd2ra, rootaffix_num)\n",
    "    return Preprocessed(\n",
    "        wd_sems=wd_sems,\n",
    "        wd_lex=wd_lex,\n",
    "        wd_ra=wd_ra,\n",
    "        mask_s=mask_s,\n",
    "        mask_l=mask_l,\n",
    "        mask_r=mask_r,\n",
    "    )\n"
   ],
   "id": "cc97b7b4efc91b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_encoder(loaded: LoadedData):\n",
    "    sememe_num = len(loaded.core_mappings.index2sememe)\n",
    "    lexname_num = len(loaded.core_mappings.index2lexname)\n",
    "    rootaffix_num = len(loaded.core_mappings.index2rootaffix)\n",
    "    model = Encoder(\n",
    "        vocab_size=len(loaded.word_mappings.word2index), \n",
    "        embed_dim=loaded.word_mappings.word2vec.shape[1], \n",
    "        hidden_dim=300, \n",
    "        layers=1, \n",
    "        class_num=loaded.label.size, \n",
    "        sememe_num=sememe_num, \n",
    "        lexname_num=lexname_num, \n",
    "        rootaffix_num=rootaffix_num,\n",
    "    )\n",
    "    model.embedding.weight.data = torch.from_numpy(loaded.word_mappings.word2vec)\n",
    "    return model"
   ],
   "id": "fb6e7d910eceb03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import TypeVar\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "CACHE_DISABLED = False\n",
    "CACHE_DIR = \"./data/cache\"\n",
    "\n",
    "def acquire(cache_filename: str, loader: Callable[[], T]) -> T:\n",
    "    if CACHE_DISABLED:\n",
    "        return loader()\n",
    "    cache_file = Path(CACHE_DIR) / cache_filename\n",
    "    if cache_file.is_file():\n",
    "        item = torch.load(str(cache_file), map_location=device)\n",
    "        print(\"loaded from cache\", cache_file)\n",
    "        return item\n",
    "    item = loader()\n",
    "    cache_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "    torch.save(item, str(cache_file))\n",
    "    print(\"cached item to\", cache_file.as_posix(), cache_file.stat().st_size, \"bytes\")\n",
    "    return item"
   ],
   "id": "20c68955bd4c3e8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "FREQUENCY = 20\n",
    "LOADED_DATA = acquire(\"loaded_data.pth\", lambda: load_data(frequency=FREQUENCY))"
   ],
   "id": "db0c4997f868f1f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "PREPROCESSED_DATA = acquire(\"preprocessed.pth\", lambda: prepare_data(LOADED_DATA))\n",
    "print(\"\\n\".join(f\"{k} = {v}\" for k, v in PREPROCESSED_DATA.describe().items()))\n",
    "# PREPROCESSED_DATA.to(device)"
   ],
   "id": "cd61216e0578972c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check(loaded_data: LoadedData, pp: Preprocessed):\n",
    "    datasets = Split.from_loaded(loaded_data)\n",
    "    ld = Loaders.from_split(datasets, batch_size=128)\n",
    "    words_t, definition_words_t = next(iter(ld.train_dataloader))\n",
    "    print(\"words_t\", words_t.shape, getattr(words_t, \"device\", None))\n",
    "    print(\"definition_words_t\", definition_words_t.shape, getattr(definition_words_t, \"device\", None))\n",
    "    model = build_encoder(loaded_data)\n",
    "    model.to(device)\n",
    "    loss, _, indices = model('train', x=definition_words_t, w=words_t, ws=pp.wd_sems, wl=pp.wd_lex, wr=pp.wd_ra, msk_s=pp.mask_s, msk_l=pp.mask_l, msk_r=pp.mask_r, mode='b')\n",
    "    print(\"loss\", loss.shape)\n",
    "    print(\"indices\", indices.shape)\n",
    "\n",
    "check(LOADED_DATA, PREPROCESSED_DATA)"
   ],
   "id": "6b8f75aee01348a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch, os, random, json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "RUN = False\n",
    "FORCE_GC = False\n",
    "\n",
    "if FORCE_GC:\n",
    "    import gc\n",
    "else:\n",
    "    class Noop:\n",
    "        def collect(self):\n",
    "            pass\n",
    "    gc = Noop()\n",
    "\n",
    "\n",
    "class Trajectory(NamedTuple):\n",
    "    \n",
    "    loss: list[float]\n",
    "    accuracy: list[dict[int, float]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create() -> 'Trajectory':\n",
    "        return Trajectory([], [])\n",
    "\n",
    "\n",
    "class History(NamedTuple):\n",
    "    \n",
    "    train: Trajectory\n",
    "    valid: Trajectory\n",
    "    \n",
    "    @staticmethod\n",
    "    def create() -> 'History':\n",
    "        return History(Trajectory.create(), Trajectory.create())\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"train\": self.train._asdict(),\n",
    "            \"valid\": self.valid._asdict(),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(d: dict[str, Any]) -> 'History':\n",
    "        return History(**dict((k, Trajectory(**v)) for k, v in d.items()))\n",
    "        \n",
    "\n",
    "\n",
    "class Restoration(NamedTuple):\n",
    "    \n",
    "    state_dict: dict[str, Any]\n",
    "    epoch: int\n",
    "    history: History\n",
    "    \n",
    "\n",
    "\n",
    "class Checkpointer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 checkpoint_dir: Path, \n",
    "                 history: History, \n",
    "                 index2word: np.ndarray, \n",
    "                 mode: str,\n",
    "                 uploader: Uploader):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.history = history\n",
    "        self.mode = mode\n",
    "        self.index2word = index2word\n",
    "        self.uploader = uploader\n",
    "    \n",
    "    def save_results(self, label_list: list[int], pred_list: list[list[int]], epoch: int):\n",
    "        saved_files = []\n",
    "        for infix, content in {\n",
    "            \"label\": (self.index2word[label_list]).tolist(),\n",
    "            \"pred\": (self.index2word[np.array(pred_list)]).tolist()\n",
    "        }.items():\n",
    "            list_file = self.checkpoint_dir / f\"checkpoint-epoch{epoch:02d}-{self.mode}_{infix}_list.json\"\n",
    "            with open(list_file, \"w\") as ofile:\n",
    "                json.dump(content, ofile, indent=2)\n",
    "            saved_files.append(list_file)\n",
    "        return saved_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def restore(checkpoint_file: Path, pt_device: str = None) -> 'Restoration':\n",
    "        checkpoint = torch.load(str(checkpoint_file), map_location=pt_device)\n",
    "        history = History.from_dict(checkpoint[\"history\"])\n",
    "        checkpoint[\"history\"] = history\n",
    "        checkpoint = dict((k, v) for k, v in checkpoint.items() if k in Restoration._fields)\n",
    "        return Restoration(**checkpoint)\n",
    "    \n",
    "    def checkpoint(self, model: torch.nn.Module, epoch: int) -> Path:\n",
    "        saved_files = {}\n",
    "        epoch_infix = f\"epoch{epoch:02d}\"\n",
    "        checkpoint_file = self.checkpoint_dir / f\"model-{epoch_infix}.pt\"\n",
    "        checkpoint_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"history\": self.history.to_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, str(checkpoint_file))\n",
    "        saved_files[\"checkpoint\"] = checkpoint_file\n",
    "        for conservation_group, file in saved_files.items():\n",
    "            self.uploader.upload_file(file, file.name, suppress_error=True, conserve=conservation_group)\n",
    "        return checkpoint_file\n",
    "\n",
    "def timestamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "\n",
    "def main(loaded: LoadedData, ld: Loaders, pp: Preprocessed, epoch_num: int, quiet: bool, MODE: str = 'b'):\n",
    "    index2word = np.array(loaded.word_mappings.index2word)\n",
    "    model = build_encoder(loaded)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam\n",
    "    best_valid_accu = 0\n",
    "    history = History.create()\n",
    "    checkpointer = Checkpointer(Path(\"./checkpoints\") / timestamp(), history, index2word, mode=MODE, uploader=UPLOADER)\n",
    "    checkpoint_file = None\n",
    "    DEF_UPDATE = True\n",
    "    for epoch in range(epoch_num):\n",
    "        print('epoch: ', epoch)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        label_list = list()\n",
    "        pred_list = list()\n",
    "            \n",
    "        for words_t, definition_words_t in tqdm(ld.train_dataloader, disable=quiet):\n",
    "            optimizer.zero_grad()\n",
    "            loss, _, indices = model('train', x=definition_words_t, w=words_t, ws=pp.wd_sems, wl=pp.wd_lex, wr=pp.wd_ra, msk_s=pp.mask_s, msk_l=pp.mask_l, msk_r=pp.mask_r, mode=MODE)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            predicted = indices[:, :100].detach().cpu().numpy().tolist()\n",
    "            train_loss += loss.item()\n",
    "            label_list.extend(words_t.detach().cpu().numpy())\n",
    "            pred_list.extend(predicted)\n",
    "        train_accu_1, train_accu_10, train_accu_100 = evaluate(label_list, pred_list)\n",
    "        del label_list\n",
    "        del pred_list\n",
    "        gc.collect()\n",
    "        train_loss_mean = train_loss/ld.counts.train\n",
    "        history.train.loss.append(train_loss_mean)\n",
    "        history.train.accuracy.append({1: train_accu_1, 10: train_accu_10, 100: train_accu_100})\n",
    "        print('train_loss: ', train_loss_mean)\n",
    "        print('train_accu(1/10/100): %.2f %.2F %.2f'%(train_accu_1, train_accu_10, train_accu_100))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            label_list = []\n",
    "            pred_list = []\n",
    "            for words_t, definition_words_t in tqdm(ld.valid_dataloader, disable=quiet):\n",
    "                loss, _, indices = model('train', x=definition_words_t, w=words_t, ws=pp.wd_sems, wl=pp.wd_lex, wr=pp.wd_ra, msk_s=pp.mask_s, msk_l=pp.mask_l, msk_r=pp.mask_r, mode=MODE)\n",
    "                predicted = indices[:, :100].detach().cpu().numpy().tolist()\n",
    "                valid_loss += loss.item()\n",
    "                label_list.extend(words_t.detach().cpu().numpy())\n",
    "                pred_list.extend(predicted)\n",
    "            valid_accu_1, valid_accu_10, valid_accu_100 = evaluate(label_list, pred_list)\n",
    "            valid_loss_mean = valid_loss/ld.counts.valid\n",
    "            history.valid.loss.append(valid_loss_mean)\n",
    "            history.valid.accuracy.append({1: valid_accu_1, 10: valid_accu_10, 100: valid_accu_100})\n",
    "            print('valid_loss: ', valid_loss_mean)\n",
    "            print('valid_accu(1/10/100): %.2f %.2F %.2f'%(valid_accu_1, valid_accu_10, valid_accu_100))\n",
    "            \n",
    "            if valid_accu_10 > best_valid_accu:\n",
    "                best_valid_accu = valid_accu_10\n",
    "                print('-----best_valid_accu-----')\n",
    "                checkpoint_file = checkpointer.checkpoint(model, epoch)\n",
    "            del label_list\n",
    "            del pred_list\n",
    "            gc.collect()\n",
    "    return checkpoint_file\n",
    "            \n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def example():\n",
    "    seed = 543624\n",
    "    setup_seed(seed)\n",
    "    datasets = Split.from_loaded(LOADED_DATA)\n",
    "    batch_size = 128\n",
    "    loaders = Loaders.from_split(datasets, batch_size=batch_size)\n",
    "    print(f'DataLoaders prepared. Batch_size {batch_size}')\n",
    "    checkpoint_file = main(\n",
    "        loaded=LOADED_DATA,\n",
    "        ld=loaders,\n",
    "        pp=PREPROCESSED_DATA,\n",
    "        epoch_num=25,\n",
    "        quiet=False,\n",
    "        MODE='b',\n",
    "    )\n",
    "    return checkpoint_file\n",
    "\n",
    "CHECKPOINT_FILE = None\n",
    "if RUN:\n",
    "    CHECKPOINT_FILE = example()\n"
   ],
   "id": "a459bee017a9f1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from typing import Tuple\n",
    "\n",
    "class CurveData(NamedTuple):\n",
    "    \n",
    "    subject: str\n",
    "    y_label: str\n",
    "    train: Sequence[float]\n",
    "    valid: Sequence[float]\n",
    "    y_bounds: Optional[Tuple[float, float]] = None\n",
    "    \n",
    "\n",
    "\n",
    "def plot_epochs_curves(history: History, title: Optional[str] = None, rank_threshold: int = 10):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    fig: Figure\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    loss = CurveData(\"Loss\", \"Cross-Entropy Loss\", history.train.loss, history.valid.loss, )\n",
    "    train_acc = np.array([acc[rank_threshold] for acc in history.train.accuracy])\n",
    "    valid_acc = np.array([acc[rank_threshold] for acc in history.valid.accuracy])\n",
    "    acc = CurveData(f\"Rank-{rank_threshold} Accuracy\", \"Correct (%)\", train_acc, valid_acc, (0.0, 100.0))\n",
    "    for ax, curve in zip(axes, [loss, acc]):\n",
    "        curve: CurveData\n",
    "        ax: Axes\n",
    "        ax.set_title(curve.subject)\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(curve.y_label)\n",
    "        train_values, val_values = curve.train, curve.valid\n",
    "        train_values, val_values = np.array(train_values), np.array(val_values)\n",
    "        epochs = list(range(max(len(train_values), len(val_values))))\n",
    "        ax.plot(epochs, train_values, label=f\"Train {curve.subject}\")\n",
    "        ax.plot(epochs, val_values, label=f\"Validation {curve.subject}\")\n",
    "        ax.legend()\n",
    "        if curve.y_bounds is not None:\n",
    "            ax.set_ylim(*curve.y_bounds)\n",
    "    plt.show()\n",
    "\n",
    "def show_checkpoint_plots(checkpoint_file: Path):\n",
    "    if not checkpoint_file or not checkpoint_file.exists():\n",
    "        print(\"checkpoint file not defined or not found:\", checkpoint_file)\n",
    "    restored = Checkpointer.restore(checkpoint_file, pt_device=\"cpu\")\n",
    "    plot_epochs_curves(restored.history)\n",
    "\n",
    "# CHECKPOINT_FILE = Path(\"./checkpoints/20240502-2111/model-epoch05.pt\")\n",
    "show_checkpoint_plots(CHECKPOINT_FILE)"
   ],
   "id": "d97e1b54d1e1851d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FullStack(NamedTuple):\n",
    "    \n",
    "    model: torch.nn.Module\n",
    "    loaded_data: LoadedData\n",
    "    preprocessed: Preprocessed\n",
    "    \n",
    "    @staticmethod\n",
    "    def instantiate(checkpoint_file: Path, pt_device: str = None) -> 'FullStack':\n",
    "        checkpoint = Checkpointer.restore(checkpoint_file, pt_device=pt_device)\n",
    "        try:\n",
    "            loaded_data = LOADED_DATA\n",
    "        except NameError:\n",
    "            loaded_data = acquire(\"loaded_data.pth\", lambda: load_data(frequency=FREQUENCY))\n",
    "        try:\n",
    "            pp = PREPROCESSED_DATA\n",
    "        except NameError:\n",
    "            pp = acquire(\"preprocessed.pth\", lambda: prepare_data(loaded_data))\n",
    "        model = build_encoder(loaded_data)\n",
    "        model.load_state_dict(checkpoint.state_dict)\n",
    "        model.to(pt_device)\n",
    "        model.eval()\n",
    "        return FullStack(model, loaded_data, pp)\n",
    "            \n",
    "        "
   ],
   "id": "16788f93b8bd666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate_test_set(checkpoint_file: Path, pt_device: str = None, quiet: bool = False, MODE: str = 'b'):\n",
    "    try:\n",
    "        stack = FullStack.instantiate(checkpoint_file, pt_device=pt_device)    \n",
    "    except FileNotFoundError:\n",
    "        print(\"checkpoint file does not exist:\", checkpoint_file.as_posix())\n",
    "        return\n",
    "    split = Split.from_loaded(stack.loaded_data)\n",
    "    dataloader = Loaders.from_split(split, batch_size=512).test_dataloader\n",
    "    pp = stack.preprocessed\n",
    "    model = stack.model\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    for words_t, definition_words_t in tqdm(dataloader, disable=quiet):\n",
    "        indices = model('test', x=definition_words_t, w=words_t, ws=pp.wd_sems, wl=pp.wd_lex, wr=pp.wd_ra, msk_s=pp.mask_s, msk_l=pp.mask_l, msk_r=pp.mask_r, mode=MODE)\n",
    "        predicted = indices[:, :1000].detach().cpu().numpy().tolist()\n",
    "        label_list.extend(words_t.detach().cpu().numpy())\n",
    "        pred_list.extend(predicted)\n",
    "    test_accu_1, test_accu_10, test_accu_100, median, variance = evaluate_test(label_list, pred_list)\n",
    "    print('test_accu(1/10/100): %.2f %.2F %.2f %.2f %.2f'%(test_accu_1, test_accu_10, test_accu_100, median, variance))\n",
    "\n",
    "# CHECKPOINT_FILE = \"./checkpoints/[...]\"\n",
    "evaluate_test(CHECKPOINT_FILE, device)\n"
   ],
   "id": "ad4d254c08693304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tabulate\n",
    "\n",
    "def user_demo(checkpoint_file: Path, pt_device: str = None, MODE: str = 'b'):\n",
    "    try:\n",
    "        stack = FullStack.instantiate(checkpoint_file, pt_device=pt_device)    \n",
    "    except FileNotFoundError:\n",
    "        print(\"checkpoint file does not exist:\", checkpoint_file.as_posix())\n",
    "        return\n",
    "    loaded_data = stack.loaded_data\n",
    "    pp = stack.preprocessed\n",
    "    model = stack.model\n",
    "    print(\"word mappings\", loaded_data.word_mappings.describe())\n",
    "    print(\"core mappings\", loaded_data.core_mappings.describe())\n",
    "    print(\"label\", loaded_data.label)\n",
    "    split = Split.from_loaded(loaded_data)\n",
    "    dataloader = Loaders.from_split(split, batch_size=8).test_dataloader\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    words_t, definition_words_t = next(iter(dataloader))\n",
    "    words_t: Tensor\n",
    "    print(f'words_t: {words_t.shape} {words_t.dtype}')\n",
    "    print(f'definition_words_t: {definition_words_t.shape}')\n",
    "    indices = model('test', x=definition_words_t, w=words_t, ws=pp.wd_sems, wl=pp.wd_lex, wr=pp.wd_ra, msk_s=pp.mask_s, msk_l=pp.mask_l, msk_r=pp.mask_r, mode=MODE)\n",
    "    predicted: list[list[int]] = indices[:, :1000].detach().cpu().numpy().tolist()\n",
    "    label_list.extend(words_t.detach().cpu().numpy())\n",
    "    def _describe(seq):\n",
    "        a = np.array(seq)\n",
    "        return f\"{a.shape} {a.dtype}\"\n",
    "    print(\"label list\", _describe(label_list))\n",
    "    pred_list.extend(predicted)\n",
    "    print(\"predicted\", _describe(predicted))\n",
    "    print(\"pred_list\", _describe(pred_list))\n",
    "    \n",
    "    table = []\n",
    "    for index, (word_i, definition_i) in enumerate(zip(words_t, definition_words_t)):\n",
    "        definition_cat = loaded_data.word_mappings.expand(definition_i)\n",
    "        prediction_i = predicted[index][0]\n",
    "        prediction_word = loaded_data.word_mappings.index2word[prediction_i]\n",
    "        try:\n",
    "            rank = predicted[index].index(word_i) + 1            \n",
    "        except ValueError:\n",
    "            rank = \"1000+\"\n",
    "        table.append([prediction_word, loaded_data.word_mappings.index2word[word_i], rank, definition_cat])\n",
    "    print()\n",
    "    print()\n",
    "    print(tabulate.tabulate(table, headers=[\"prediction\", \"actual\", \"actual\\nrank\", \"clue\"]))\n",
    "        \n",
    "\n",
    "\n",
    "# CHECKPOINT_FILE = \"./checkpoints/[...]\"\n",
    "user_demo(CHECKPOINT_FILE, device)"
   ],
   "id": "56add5cf467fec42",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
