{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MultiRD",
   "id": "9d85d4123d01cc83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# unpack data\n",
    "!(test -d data || (gdown \"1zjrLaaKR9Pf-DUmjkoptRyG4SBasgked\" && unzip -q \"english-rd-data.zip\"))\n",
    "!ls data\n",
    "DATA_PATH = './data'"
   ],
   "id": "5d7b696edfbd4517"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "data.py",
   "id": "13848d4e2669b03e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.cuda\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from typing import TextIO\n",
    "from typing import Any\n",
    "from json import load as json_load\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, instances):\n",
    "        self.instances = instances\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.instances[index]\n",
    " \n",
    "def data2index(data_x, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"word\": \"restlessly\",\n",
    "        \"lexnames\": [\n",
    "            \"adv.all\"\n",
    "        ],\n",
    "        \"root_affix\": [\n",
    "            \"ly\"\n",
    "        ],\n",
    "        \"sememes\": [\n",
    "            \"rash\"\n",
    "        ],\n",
    "        \"definitions\": \"in a restless manner unquietly\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    data_x_idx = list()\n",
    "    for instance in data_x:\n",
    "        sememe_idx = [sememe2index[se] for se in instance['sememes']]\n",
    "        lexname_idx = [lexname2index[ln] for ln in instance['lexnames']]\n",
    "        rootaffix_idx = [rootaffix2index[ra] for ra in instance['root_affix'] if rootaffix_freq[ra]>=frequency]\n",
    "        def_word_idx = list()\n",
    "        def_words = instance['definitions'].strip().split()\n",
    "        if len(def_words) > 0:\n",
    "            for def_word in def_words:\n",
    "                if def_word in word2index and def_word!=instance['word']:\n",
    "                    def_word_idx.append(word2index[def_word])\n",
    "                else:\n",
    "                    def_word_idx.append(word2index['<OOV>'])\n",
    "            data_x_idx.append({'word': word2index[instance['word']], 'lexnames': lexname_idx, 'root_affix':rootaffix_idx, 'sememes': sememe_idx, 'definition_words': def_word_idx})\n",
    "        else:\n",
    "            pass #print(instance['word'], instance['definitions']) # some is null\n",
    "    return data_x_idx\n",
    "\n",
    "\n",
    "def readlines(f: TextIO):\n",
    "    return f.readlines()\n",
    "\n",
    "\n",
    "def load_data_file(filename: str, transform: Callable[[TextIO], Any]):\n",
    "    with open(os.path.join(DATA_PATH, filename)) as ifile:\n",
    "        if transform is not None:\n",
    "            return transform(ifile)\n",
    "        return ifile.read()\n",
    "\n",
    "def load_data_json(filename: str):\n",
    "    return load_data_file(filename, transform=json_load)\n",
    "\n",
    "def load_data_lines(filename: str):\n",
    "    return load_data_file(filename, transform=readlines)\n",
    "\n",
    "\n",
    "def load_data(frequency):\n",
    "    print('Loading dataset...')\n",
    "    data_train = load_data_json(\"data_train.json\")\n",
    "    data_dev = load_data_json(\"data_dev.json\")\n",
    "    data_test_500_rand1_seen = load_data_json(\"data_test_500_rand1_seen.json\")\n",
    "    data_test_500_rand1_unseen = load_data_json(\"data_test_500_rand1_unseen.json\") #data_test_500_others\n",
    "    data_defi_c = load_data_json(\"data_defi_c.json\")\n",
    "    data_desc_c = load_data_json(\"data_desc_c.json\")\n",
    "    lines = load_data_lines(\"target_words.txt\")\n",
    "    target_words = [line.strip() for line in lines]\n",
    "    label_size = len(target_words)+2\n",
    "    print('target_words (include <PAD><OOV>): ', label_size)\n",
    "    lines = load_data_lines(\"lexname_all.txt\")\n",
    "    lexname_all = [line.strip() for line in lines]\n",
    "    label_lexname_size = len(lexname_all)\n",
    "    print('label_lexname_size: ', label_lexname_size)\n",
    "    lines = load_data_lines(\"root_affix_freq.txt\")\n",
    "    rootaffix_freq = {}\n",
    "    for line in lines:\n",
    "        rootaffix_freq[line.strip().split()[0]] = int(line.strip().split()[1])\n",
    "    lines = load_data_lines(\"rootaffix_all.txt\")\n",
    "    rootaffix_all = [line.strip() for line in lines]\n",
    "    lines = load_data_lines(\"sememes_all.txt\")\n",
    "    sememes_all = [line.strip() for line in lines]\n",
    "    label_sememe_size = len(sememes_all)+1\n",
    "    print('label_sememe_size: ', label_sememe_size)\n",
    "    vec_inuse = load_data_json(\"vec_inuse.json\")\n",
    "    vocab = list(vec_inuse)\n",
    "    vocab_size = len(vocab)+2\n",
    "    print('vocab (embeddings in use)(include <PAD><OOV>): ', vocab_size)\n",
    "    word2index = dict()\n",
    "    index2word = list()\n",
    "    word2index['<PAD>'] = 0\n",
    "    word2index['<OOV>'] = 1\n",
    "    index2word.extend(['<PAD>', '<OOV>'])\n",
    "    index2word.extend(vocab)\n",
    "    word2vec = np.zeros((vocab_size, len(list(vec_inuse.values())[0])), dtype=np.float32)\n",
    "    for wd in target_words: \n",
    "        index = len(word2index)\n",
    "        word2index[wd] = index\n",
    "        word2vec[index, :] = vec_inuse[wd]\n",
    "    for wd in vocab:\n",
    "        if wd in target_words:\n",
    "            continue\n",
    "        index = len(word2index)\n",
    "        word2index[wd] = index\n",
    "        word2vec[index, :] = vec_inuse[wd]\n",
    "    sememe2index = dict()\n",
    "    index2sememe = list()\n",
    "    for sememe in sememes_all:\n",
    "        sememe2index[sememe] = len(sememe2index)\n",
    "        index2sememe.append(sememe)\n",
    "    lexname2index = dict()\n",
    "    index2lexname = list()\n",
    "    for ln in lexname_all:\n",
    "        lexname2index[ln] = len(lexname2index)\n",
    "        index2lexname.append(ln)\n",
    "    rootaffix2index = dict()\n",
    "    index2rootaffix = list()\n",
    "    for ra in rootaffix_all:\n",
    "        if rootaffix_freq[ra] >= frequency:\n",
    "            rootaffix2index[ra] = len(rootaffix2index)\n",
    "            index2rootaffix.append(ra)\n",
    "    label_rootaffix_size = len(index2rootaffix)\n",
    "    print('label_rootaffix_size: ', label_rootaffix_size)\n",
    "    data_train_idx = data2index(data_train, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    print('data_train size: %d'%len(data_train_idx))\n",
    "    data_dev_idx = data2index(data_dev, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    print('data_dev size: %d'%len(data_dev_idx))\n",
    "    data_test_500_seen_idx = data2index(data_test_500_rand1_seen, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency) \n",
    "    print('data_test_seen size: %d'%len(data_test_500_seen_idx))\n",
    "    data_test_500_unseen_idx = data2index(data_test_500_rand1_unseen, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency) \n",
    "    print('data_test_unseen size: %d'%len(data_test_500_unseen_idx))\n",
    "    data_defi_c_idx = data2index(data_defi_c, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)\n",
    "    data_desc_c_idx = data2index(data_desc_c, word2index, sememe2index, lexname2index, rootaffix2index, rootaffix_freq, frequency)    \n",
    "    print('data_desc size: %d'%len(data_desc_c_idx))\n",
    "    return word2index, index2word, word2vec, (index2sememe, index2lexname, index2rootaffix), (label_size, label_lexname_size, label_rootaffix_size, label_sememe_size), (data_train_idx, data_dev_idx, data_test_500_seen_idx, data_test_500_unseen_idx, data_defi_c_idx, data_desc_c_idx)\n",
    "\n",
    "    \n",
    "def build_sentence_numpy(sentences):\n",
    "    max_length = max([len(sentence) for sentence in sentences])\n",
    "    sentence_numpy = np.zeros((len(sentences), max_length), dtype=np.int64)\n",
    "    for i in range(len(sentences)):\n",
    "        sentence_numpy[i, 0:len(sentences[i])] = np.array(sentences[i])\n",
    "    return sentence_numpy\n",
    "    \n",
    "\n",
    "def label_multihot(labels, num):\n",
    "    sm = np.zeros((len(labels), num), dtype=np.float32)\n",
    "    for i in range(len(labels)):\n",
    "        for s in labels[i]:\n",
    "            if s >= num:\n",
    "                break\n",
    "            sm[i, s] = 1\n",
    "    return sm\n",
    "    \n",
    "def my_collate_fn(batch):\n",
    "    words = [instance['word'] for instance in batch]\n",
    "    definition_words = [instance['definition_words'] for instance in batch]\n",
    "    words_t = torch.tensor(np.array(words), dtype=torch.int64, device=device)\n",
    "    definition_words_t = torch.tensor(build_sentence_numpy(definition_words), dtype=torch.int64, device=device)\n",
    "    return words_t, definition_words_t\n",
    "    \n",
    "def word2feature(dataset, word_num, feature_num, feature_name):\n",
    "    max_feature_num = max([len(instance[feature_name]) for instance in dataset])\n",
    "    ret = np.zeros((word_num, max_feature_num), dtype=np.int64)\n",
    "    ret.fill(feature_num)\n",
    "    for instance in dataset:\n",
    "        if ret[instance['word'], 0] != feature_num: \n",
    "            continue # this target_words has been given a feature mapping, because same word with different definition in dataset\n",
    "        feature = instance[feature_name]\n",
    "        ret[instance['word'], :len(feature)] = np.array(feature)\n",
    "    return torch.tensor(ret, dtype=torch.int64, device=device)\n",
    "    \n",
    "def mask_noFeature(label_size, wd2fea, feature_num):\n",
    "    mask_nofea = torch.zeros(label_size, dtype=torch.float32, device=device)\n",
    "    for i in range(label_size):\n",
    "        feas = set(wd2fea[i].detach().cpu().numpy().tolist()) - {feature_num}\n",
    "        if len(feas)==0:\n",
    "            mask_nofea[i] = 1\n",
    "    return mask_nofea\n"
   ],
   "id": "5751f965237d32b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "model.py",
   "id": "1daeb8e189ab5d23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        # x: T(bat, len, emb) float32\n",
    "        # x_len: T(bat) int64\n",
    "        _, x_len_sort_idx = torch.sort(-x_len)\n",
    "        _, x_len_unsort_idx = torch.sort(x_len_sort_idx)\n",
    "        x = x[x_len_sort_idx]\n",
    "        x_len = x_len[x_len_sort_idx]\n",
    "        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True)\n",
    "        # ht: T(num_layers*2, bat, hid) float32\n",
    "        # ct: T(num_layers*2, bat, hid) float32\n",
    "        h_packed, (ht, ct) = self.lstm(x_packed, None)\n",
    "        ht = ht[:, x_len_unsort_idx, :]\n",
    "        ct = ct[:, x_len_unsort_idx, :]\n",
    "        # h: T(bat, len, hid*2) float32\n",
    "        h, _ = torch.nn.utils.rnn.pad_packed_sequence(h_packed, batch_first=True)\n",
    "        h = h[x_len_unsort_idx]\n",
    "        return h, (ht, ct)\n",
    "        \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, layers, class_num, sememe_num, lexname_num, rootaffix_num):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.class_num = class_num\n",
    "        self.sememe_num = sememe_num\n",
    "        self.lexname_num = lexname_num\n",
    "        self.rootaffix_num = rootaffix_num\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0, max_norm=5, sparse=True)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = torch.nn.Dropout()\n",
    "        self.encoder = BiLSTM(self.embed_dim, self.hidden_dim, self.layers)\n",
    "        self.fc = torch.nn.Linear(self.hidden_dim*2, self.embed_dim)\n",
    "        self.fc_s = torch.nn.Linear(self.hidden_dim*2, self.sememe_num)\n",
    "        self.fc_l = torch.nn.Linear(self.hidden_dim*2, self.lexname_num)\n",
    "        self.fc_r = torch.nn.Linear(self.hidden_dim*2, self.rootaffix_num)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, operation, x=None, w=None, ws=None, wl=None, wr=None, msk_s=None, msk_l=None, msk_r=None, mode=None):\n",
    "        # x: T(bat, max_word_num)\n",
    "        # w: T(bat)\n",
    "        # x_embedding: T(bat, max_word_num, embed_dim)\n",
    "        x_embedding = self.embedding(x)\n",
    "        x_embedding = self.embedding_dropout(x_embedding)\n",
    "        # mask: T(bat, max_word_num)\n",
    "        mask = torch.gt(x, 0).to(torch.int64)\n",
    "        # x_len: T(bat)\n",
    "        x_len = torch.sum(mask, dim=1)\n",
    "        # h: T(bat, max_word_num, hid*2)\n",
    "        # ht: T(num_layers*2, bat, hid) float32\n",
    "        h, (ht, _) = self.encoder(x_embedding, x_len)\n",
    "        # ht: T(bat, hid*2)\n",
    "        ht = torch.transpose(ht[ht.shape[0] - 2:, :, :], 0, 1).contiguous().view(x_len.shape[0], self.hidden_dim*2)\n",
    "        # alpha: T(bat, max_word_num, 1)\n",
    "        alpha = (h.bmm(ht.unsqueeze(2)))\n",
    "        # mask_3: T(bat, max_word_num, 1)\n",
    "        mask_3 = mask.to(torch.float32).unsqueeze(2)\n",
    "\n",
    "        ## word prediction\n",
    "        # vd: T(bat, embed_dim)\n",
    "        h_1 = torch.sum(h*alpha, 1)\n",
    "        vd = self.fc(h_1) #+ torch.sum(self.embedding(x), 1)#+ torch.sum(x_embedding, 1) #ok\n",
    "        #vd = self.fc(torch.sum(torch.cat([h, self.embedding(x)], 2)*alpha, 1)) #best\n",
    "        score0 = vd.mm(self.embedding.weight[[range(self.class_num)]].t())\n",
    "        score = score0\n",
    "        if 's' in mode:\n",
    "            ## sememe prediction\n",
    "            # pos_score: T(bat, max_word_num, sememe_num)\n",
    "            pos_score = self.fc_s(h)\n",
    "            pos_score = pos_score*mask_3 + (-1e7)*(1-mask_3)\n",
    "            # sem_score: T(bat, sememe_num)\n",
    "            sem_score, _ = torch.max(pos_score, dim=1)\n",
    "            #sem_score = torch.sum(pos_score * alpha, 1)\n",
    "            # score: T(bat, class_num) = [bat, sememe_num] .mm [class_num, sememe_num].t()\n",
    "            score_s = self.relu(sem_score.mm(ws.t()))\n",
    "            #----------add mean sememe score to those who have no sememes\n",
    "            # mean_sem_sc: T(bat)\n",
    "            mean_sem_sc = torch.mean(score_s, 1)\n",
    "            # msk: T(class_num)\n",
    "            score_s = score_s + mean_sem_sc.unsqueeze(1).mm(msk_s.unsqueeze(0))\n",
    "            #----------\n",
    "            score = score + score_s\n",
    "        if 'r' in mode:\n",
    "            ## root-affix prediction\n",
    "            pos_score_ = self.fc_r(h)\n",
    "            pos_score_ = pos_score_*mask_3 + (-1e7)*(1-mask_3)\n",
    "            ra_score, _ = torch.max(pos_score_, dim=1)\n",
    "            score_r = self.relu(ra_score.mm(wr.t()))\n",
    "            mean_ra_sc = torch.mean(score_r, 1)\n",
    "            score_r = score_r + mean_ra_sc.unsqueeze(1).mm(msk_r.unsqueeze(0))\n",
    "            score = score + score_r\n",
    "        if 'l' in mode:\n",
    "            ## lexname prediction\n",
    "            lex_score = self.fc_l(h_1)\n",
    "            score_l = self.relu(lex_score.mm(wl.t()))\n",
    "            mean_lex_sc = torch.mean(score_l, 1)\n",
    "            score_l = score_l + mean_lex_sc.unsqueeze(1).mm(msk_l.unsqueeze(0))\n",
    "            score = score + score_l\n",
    "        \n",
    "        # fine-tune depended on the target word shouldn't exist in the definition.\n",
    "        #score_res = score.clone().detach()\n",
    "        mask1 = torch.lt(x, self.class_num).to(torch.int64)\n",
    "        mask2 = torch.ones((score.shape[0], score.shape[1]), dtype=torch.float32, device=device)\n",
    "        for i in range(x.shape[0]):\n",
    "            mask2[i][x[i]*mask1[i]] = 0.\n",
    "        score = score * mask2 + (-1e6)*(1-mask2)\n",
    "        \n",
    "        _, indices = torch.sort(score, descending=True)\n",
    "        if operation == 'train':\n",
    "            loss = self.loss(score, w)\n",
    "            return loss, score, indices\n",
    "        elif operation == 'test':\n",
    "            return indices\n"
   ],
   "id": "5bf99c9d404fd1ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "evaluate.py",
   "id": "29457543a1b3afac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def evaluate(ground_truth, prediction):\n",
    "    accu_1 = 0.\n",
    "    accu_10 = 0.\n",
    "    accu_100 = 0.\n",
    "    length = len(ground_truth)\n",
    "    for i in range(length):\n",
    "        if ground_truth[i] in prediction[i][:100]:\n",
    "            accu_100 += 1\n",
    "            if ground_truth[i] in prediction[i][:10]:\n",
    "                accu_10 += 1\n",
    "                if ground_truth[i] == prediction[i][0]:\n",
    "                    accu_1 += 1\n",
    "    return accu_1/length*100, accu_10/length*100, accu_100/length*100\n",
    "\n",
    "def evaluate_test(ground_truth, prediction):\n",
    "    accu_1 = 0.\n",
    "    accu_10 = 0.\n",
    "    accu_100 = 0.\n",
    "    length = len(ground_truth)\n",
    "    pred_rank = []\n",
    "    for i in range(length):\n",
    "        try:\n",
    "            pred_rank.append(prediction[i][:].index(ground_truth[i]))\n",
    "        except:\n",
    "            pred_rank.append(1000)\n",
    "        if ground_truth[i] in prediction[i][:100]:\n",
    "            accu_100 += 1\n",
    "            if ground_truth[i] in prediction[i][:10]:\n",
    "                accu_10 += 1\n",
    "                if ground_truth[i] == prediction[i][0]:\n",
    "                    accu_1 += 1\n",
    "    return accu_1/length*100, accu_10/length*100, accu_100/length*100, np.median(pred_rank), np.sqrt(np.var(pred_rank))\n",
    "\n",
    "# '''\n",
    "# def evaluate_MAP(ground_truth, prediction):\n",
    "#     index = 1\n",
    "#     correct = 0\n",
    "#     point = 0\n",
    "#     for predicted_POS in prediction:\n",
    "#         if predicted_POS in ground_truth:\n",
    "#             correct += 1\n",
    "#             point += (correct / index)\n",
    "#         index += 1\n",
    "#     point /= len(ground_truth)\n",
    "#     return point*100.\n",
    "# \n",
    "# import numpy as np    \n",
    "# def evaluate1(ground_truth, prediction):\n",
    "#     length = len(ground_truth)\n",
    "#     ref = np.array(ground_truth)[:, np.newaxis]\n",
    "#     _, c = np.where(np.array(prediction)==ref)\n",
    "#     accu_1 = np.sum(c==0)\n",
    "#     accu_10 = np.sum(c<10)\n",
    "#     accu_100 = np.sum(c<100)\n",
    "#     return accu_1/length*100, accu_10/length*100, accu_100/length*100\n",
    "# '''"
   ],
   "id": "5fd013aa6561077a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Something else?",
   "id": "2367a9ca33c3f4dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TBD",
   "id": "389f6b38618c70bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "main.py",
   "id": "2aaa8600a06f9225"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch, os, random, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "RUN = False\n",
    "FORCE_GC = False\n",
    "\n",
    "if FORCE_GC:\n",
    "    import gc\n",
    "else:\n",
    "    class Noop:\n",
    "        def collect(self):\n",
    "            pass\n",
    "    gc = Noop()\n",
    "\n",
    "\n",
    "def main(frequency, batch_size, epoch_num, verbose, MODE):\n",
    "    mode = MODE\n",
    "    word2index, index2word, word2vec, index2each, label_size_each, data_idx_each = load_data(frequency)\n",
    "    (label_size, label_lexname_size, label_rootaffix_size, label_sememe_size) = label_size_each\n",
    "    (data_train_idx, data_dev_idx, data_test_500_seen_idx, data_test_500_unseen_idx, data_defi_c_idx, data_desc_c_idx) = data_idx_each\n",
    "    (index2sememe, index2lexname, index2rootaffix) = index2each\n",
    "    index2word = np.array(index2word)\n",
    "    test_dataset = MyDataset(data_test_500_seen_idx + data_test_500_unseen_idx + data_desc_c_idx)\n",
    "    valid_dataset = MyDataset(data_dev_idx)\n",
    "    train_dataset = MyDataset(data_train_idx + data_defi_c_idx)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "    \n",
    "    print('DataLoader prepared. Batch_size [%d]'%batch_size)\n",
    "    print('Train dataset: ', len(train_dataset))\n",
    "    print('Valid dataset: ', len(valid_dataset))\n",
    "    print('Test dataset: ', len(test_dataset))\n",
    "    data_all_idx = data_train_idx + data_dev_idx + data_test_500_seen_idx + data_test_500_unseen_idx + data_defi_c_idx\n",
    "    \n",
    "    sememe_num = len(index2sememe)\n",
    "    wd2sem = word2feature(data_all_idx, label_size, sememe_num, 'sememes') # label_size, not len(word2index). we only use target_words' feature\n",
    "    wd_sems = label_multihot(wd2sem, sememe_num)\n",
    "    wd_sems = torch.from_numpy(np.array(wd_sems)).to(device) #torch.from_numpy(np.array(wd_sems[:label_size])).to(device)\n",
    "    lexname_num = len(index2lexname)\n",
    "    wd2lex = word2feature(data_all_idx, label_size, lexname_num, 'lexnames') \n",
    "    wd_lex = label_multihot(wd2lex, lexname_num)\n",
    "    wd_lex = torch.from_numpy(np.array(wd_lex)).to(device)\n",
    "    rootaffix_num = len(index2rootaffix)\n",
    "    wd2ra = word2feature(data_all_idx, label_size, rootaffix_num, 'root_affix') \n",
    "    wd_ra = label_multihot(wd2ra, rootaffix_num)\n",
    "    wd_ra = torch.from_numpy(np.array(wd_ra)).to(device)\n",
    "    mask_s = mask_noFeature(label_size, wd2sem, sememe_num)\n",
    "    mask_l = mask_noFeature(label_size, wd2lex, lexname_num)\n",
    "    mask_r = mask_noFeature(label_size, wd2ra, rootaffix_num)\n",
    "    \n",
    "    model = Encoder(vocab_size=len(word2index), embed_dim=word2vec.shape[1], hidden_dim=300, layers=1, class_num=label_size, sememe_num=sememe_num, lexname_num=lexname_num, rootaffix_num=rootaffix_num)\n",
    "    model.embedding.weight.data = torch.from_numpy(word2vec)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam\n",
    "    best_valid_accu = 0\n",
    "    DEF_UPDATE = True\n",
    "    for epoch in range(epoch_num):\n",
    "        print('epoch: ', epoch)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        label_list = list()\n",
    "        pred_list = list()\n",
    "        for words_t, definition_words_t in tqdm(train_dataloader, disable=verbose):\n",
    "            optimizer.zero_grad()\n",
    "            loss, _, indices = model('train', x=definition_words_t, w=words_t, ws=wd_sems, wl=wd_lex, wr=wd_ra, msk_s=mask_s, msk_l=mask_l, msk_r=mask_r, mode=MODE)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            predicted = indices[:, :100].detach().cpu().numpy().tolist()\n",
    "            train_loss += loss.item()\n",
    "            label_list.extend(words_t.detach().cpu().numpy())\n",
    "            pred_list.extend(predicted)\n",
    "        train_accu_1, train_accu_10, train_accu_100 = evaluate(label_list, pred_list)\n",
    "        del label_list\n",
    "        del pred_list\n",
    "        gc.collect()\n",
    "        print('train_loss: ', train_loss/len(train_dataset))\n",
    "        print('train_accu(1/10/100): %.2f %.2F %.2f'%(train_accu_1, train_accu_10, train_accu_100))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            label_list = []\n",
    "            pred_list = []\n",
    "            for words_t, definition_words_t in tqdm(valid_dataloader, disable=verbose):\n",
    "                loss, _, indices = model('train', x=definition_words_t, w=words_t, ws=wd_sems, wl=wd_lex, wr=wd_ra, msk_s=mask_s, msk_l=mask_l, msk_r=mask_r, mode=MODE)\n",
    "                predicted = indices[:, :100].detach().cpu().numpy().tolist()\n",
    "                valid_loss += loss.item()\n",
    "                label_list.extend(words_t.detach().cpu().numpy())\n",
    "                pred_list.extend(predicted)\n",
    "            valid_accu_1, valid_accu_10, valid_accu_100 = evaluate(label_list, pred_list)\n",
    "            print('valid_loss: ', valid_loss/len(valid_dataset))\n",
    "            print('valid_accu(1/10/100): %.2f %.2F %.2f'%(valid_accu_1, valid_accu_10, valid_accu_100))\n",
    "            del label_list\n",
    "            del pred_list\n",
    "            gc.collect()\n",
    "            \n",
    "            if valid_accu_10>best_valid_accu:\n",
    "                best_valid_accu = valid_accu_10\n",
    "                print('-----best_valid_accu-----')\n",
    "                #torch.save(model, 'saved.model')\n",
    "                test_loss = 0\n",
    "                label_list = []\n",
    "                pred_list = []\n",
    "                for words_t, definition_words_t in tqdm(test_dataloader, disable=verbose):\n",
    "                    indices = model('test', x=definition_words_t, w=words_t, ws=wd_sems, wl=wd_lex, wr=wd_ra, msk_s=mask_s, msk_l=mask_l, msk_r=mask_r, mode=MODE)\n",
    "                    predicted = indices[:, :1000].detach().cpu().numpy().tolist()\n",
    "                    label_list.extend(words_t.detach().cpu().numpy())\n",
    "                    pred_list.extend(predicted)\n",
    "                test_accu_1, test_accu_10, test_accu_100, median, variance = evaluate_test(label_list, pred_list)\n",
    "                print('test_accu(1/10/100): %.2f %.2F %.2f %.2f %.2f'%(test_accu_1, test_accu_10, test_accu_100, median, variance))\n",
    "                if epoch>5:\n",
    "                    with open(mode+'_label_list.json', 'w') as ofile:\n",
    "                        json.dump((index2word[label_list]).tolist(), ofile)\n",
    "                    with open(mode+'_pred_list.json', 'w') as ofile:\n",
    "                        json.dump((index2word[np.array(pred_list)]).tolist(), ofile)\n",
    "                del label_list\n",
    "                del pred_list\n",
    "                gc.collect()\n",
    "            \n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def example():\n",
    "    seed = 543624\n",
    "    setup_seed(seed)\n",
    "    main(\n",
    "        frequency=20,\n",
    "        batch_size=128,\n",
    "        epoch_num=25,\n",
    "        verbose=True,\n",
    "        MODE='b',\n",
    "    )\n",
    "\n",
    "if RUN:\n",
    "    example()\n"
   ],
   "id": "a459bee017a9f1d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
